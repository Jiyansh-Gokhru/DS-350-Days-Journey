# Training vs Test Error

## Model Complexity vs Error

As model complexity increases:

- **Training error** always decreases  
- **Test error** first decreases, then increases

---

## High Bias (Underfitting)
- Model is too simple
- Training error: High
- Test error: High
- Cannot capture data patterns

---

## Optimal Model
- Balanced bias and variance
- Training error: Low
- Test error: Minimum
- Best generalization

---

## High Variance (Overfitting)
- Model is too complex
- Training error: Very low
- Test error: High
- Model memorizes noise

---

## Key Interview Line
“A good model is not the one with lowest training error,  
but the one with lowest test error.”
